{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Import constants and catch any import errors\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import importlib\n",
    "\n",
    "# Import constants and catch any import errors\n",
    "from link_to_constants import *\n",
    "link_constants()\n",
    "try:\n",
    "    from constants import * # type: ignore\n",
    "    print(\"Successfully imported constants.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing constants: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_points_grid_map(csv_file):\n",
    "    \"\"\"Load bounding box vertices from a CSV file.\"\"\"\n",
    "    points = []\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Extract the 3D coordinates of the 8 bounding box vertices\n",
    "            coordinates = [\n",
    "                float(row[0]), float(row[1]), float(row[2])\n",
    "                ]\n",
    "            points.append(coordinates)\n",
    "    np_points = np.array(points)\n",
    "    return np_points\n",
    "\n",
    "def generate_grid_map (grid_map_path):\n",
    "    grid_map_files = [f for f in os.listdir(grid_map_path)]\n",
    "    list_grid_maps = []\n",
    "\n",
    "    for file in grid_map_files:\n",
    "        complete_path = os.path.join(grid_map_path, file)\n",
    "        print(f\"Loading {file}...\")\n",
    "        points = load_points_grid_map(complete_path)\n",
    "\n",
    "        # Recreate the grid map from positions array\n",
    "        grid_map_recreate = np.full((Y_RANGE, X_RANGE), FLOOR_HEIGHT, dtype=float) # type: ignore\n",
    "\n",
    "        # Fill the grid map with values from positions array\n",
    "        for pos in points:\n",
    "            col, row, height = pos\n",
    "            grid_map_recreate[int(row), int(col)] = height\n",
    "\n",
    "        list_grid_maps.append(grid_map_recreate)\n",
    "    \n",
    "    return list_grid_maps\n",
    "\n",
    "def load_points_grid_map_BB (csv_file):\n",
    "    \"\"\"Load bounding box vertices from a CSV file.\"\"\"\n",
    "    points = []\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip header\n",
    "        \n",
    "        for row in reader:\n",
    "            # Extract the 3D coordinates of the 8 bounding box vertices\n",
    "            coordinates = [ [ float(row[i]), float(row[i+1]), float(row[i+2]) ] for i in range(2, 12, 3)]\n",
    "            points.append(coordinates)\n",
    "\n",
    "    np_points = np.array(points)\n",
    "    return np_points\n",
    "\n",
    "def generate_grid_map_BB (grid_map_path):\n",
    "    grid_map_files = [f for f in os.listdir(grid_map_path)]\n",
    "    list_grid_maps = []\n",
    "    list_num_BB = []    \n",
    "\n",
    "    for file in grid_map_files:\n",
    "        complete_path = os.path.join(grid_map_path, file)\n",
    "        print(f\"Loading {file}...\")\n",
    "        points = load_points_grid_map_BB(complete_path)\n",
    "\n",
    "        num_BB = points.shape[0]\n",
    "\n",
    "        # Recreate the grid map from positions array\n",
    "        grid_map_recreate = np.full((Y_RANGE, X_RANGE), FLOOR_HEIGHT, dtype=float) # type: ignore\n",
    "\n",
    "        # Fill the grid map with values from positions array\n",
    "        for i in range (len(points)):\n",
    "            for j in range(4):\n",
    "                col, row, height = points[i][j]\n",
    "                grid_map_recreate[int(row), int(col)] = height\n",
    "\n",
    "        list_grid_maps.append(grid_map_recreate)\n",
    "        list_num_BB.append(num_BB) #used to stratify the dataset\n",
    "        \n",
    "    return list_grid_maps, list_num_BB\n",
    "\n",
    "def split_data(lidar_data, BB_data, num_BB, size):\n",
    "    # Split the dataset into a combined training and validation set, and a separate test set using num_BB as stratification\n",
    "    X_train_val, X_test, y_train_val, y_test, num_BB_train_val, num_BB_test = train_test_split(\n",
    "        lidar_data, # Samples\n",
    "        BB_data, # Labels\n",
    "        num_BB, # Number of BB\n",
    "        test_size = size,\n",
    "        random_state=SEED, # type: ignore\n",
    "        stratify=num_BB\n",
    "    )\n",
    "    return X_train_val, X_test, y_train_val, y_test, num_BB_train_val, num_BB_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_grid_maps = []\n",
    "complete_grid_maps_BB = []\n",
    "complete_num_BB = []\n",
    "\n",
    "# Load sensor1\n",
    "grid_maps = generate_grid_map(LIDAR_1_GRID_DIRECTORY) # type: ignore\n",
    "grid_maps_BB, num_BB  = generate_grid_map_BB(NEW_POSITIONS_LIDAR_1_GRID_DIRECTORY) # type: ignore\n",
    "\n",
    "complete_grid_maps.append(grid_maps)\n",
    "complete_grid_maps_BB.append(grid_maps_BB)\n",
    "complete_num_BB.append(num_BB)\n",
    "\n",
    "# Load sensor2\n",
    "grid_maps = generate_grid_map(LIDAR_2_GRID_DIRECTORY) # type: ignore\n",
    "grid_maps_BB, num_BB = generate_grid_map_BB(NEW_POSITIONS_LIDAR_2_GRID_DIRECTORY) # type: ignore\n",
    "\n",
    "complete_grid_maps.append(grid_maps)\n",
    "complete_grid_maps_BB.append(grid_maps_BB)\n",
    "complete_num_BB.append(num_BB)\n",
    "\n",
    "# Load sensor3\n",
    "grid_maps = generate_grid_map(LIDAR_3_GRID_DIRECTORY) # type: ignore\n",
    "grid_maps_BB, num_BB = generate_grid_map_BB(NEW_POSITIONS_LIDAR_3_GRID_DIRECTORY) # type: ignore\n",
    "\n",
    "complete_grid_maps.append(grid_maps)\n",
    "complete_grid_maps_BB.append(grid_maps_BB)\n",
    "complete_num_BB.append(num_BB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(complete_num_BB[0]), len(complete_num_BB[1]), len(complete_num_BB[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_num_BB_np = np.concatenate(complete_num_BB, axis=0)\n",
    "print(complete_num_BB_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_num_BB_np = np.expand_dims(complete_num_BB_np, axis=1)\n",
    "print(complete_num_BB_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(complete_grid_maps[0]), len(complete_grid_maps[1]), len(complete_grid_maps[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(complete_grid_maps_BB[0]), len(complete_grid_maps_BB[1]), len(complete_grid_maps_BB[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the lists in complete_grid_maps along the first dimension\n",
    "complete_grid_maps_np = np.concatenate(complete_grid_maps, axis=0)\n",
    "print(complete_grid_maps_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the lists in complete_grid_maps_BB along the first dimension\n",
    "complete_grid_maps_BB_np = np.concatenate(complete_grid_maps_BB, axis=0)\n",
    "print(complete_grid_maps_BB_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_grid_maps_np[0][200][200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train_val, X_test, y_train_val, y_test, num_BB_train_val, num_BB_test = split_data(complete_grid_maps_np, complete_grid_maps_BB_np, complete_num_BB_np, TEST_SIZE) # type: ignore\n",
    "X_train, X_val, y_train, y_val, num_BB_train, num_BB_val = split_data(X_train_val, y_train_val, num_BB_train_val, len(X_test)) # Esure that val and test set have the same lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape, num_BB_train.shape, num_BB_val.shape, num_BB_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_train = 0\n",
    "sum_val = 0\n",
    "sum_test = 0\n",
    "for i in range(len(num_BB_train)):\n",
    "    sum_train += num_BB_train[i]\n",
    "print(f\"Sum_train: \", sum_train)\n",
    "print(f\"Average_sum_train: \",sum_train/len(num_BB_train))\n",
    "\n",
    "for i in range(len(num_BB_val)):\n",
    "    sum_val += num_BB_val[i]\n",
    "print(f\"Sum_val: \",sum_val)\n",
    "print(f\"Average_sum_val: \",sum_val/len(num_BB_val))\n",
    "\n",
    "for i in range(len(num_BB_test)):\n",
    "    sum_test += num_BB_test[i]\n",
    "print(f\"Sum_test: \",sum_test)\n",
    "print(f\"Average_sum_test: \",sum_test/len(num_BB_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize the data\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_val = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train.reshape(-1, y_train.shape[-1])).reshape(y_train.shape)\n",
    "y_val = scaler.transform(y_val.reshape(-1, y_val.shape[-1])).reshape(y_val.shape)\n",
    "y_test = scaler.transform(y_test.reshape(-1, y_test.shape[-1])).reshape(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_val = np.expand_dims(X_val, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "print(\"New input shape (train, val, test):\", X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.expand_dims(y_train, axis=1)\n",
    "y_val = np.expand_dims(y_val, axis=1)\n",
    "y_test = np.expand_dims(y_test, axis=1)\n",
    "print(\"New labels shape (train, val, test):\", y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self): # Constructor method for the autoencoder\n",
    "        super(Autoencoder, self).__init__() # Calls the constructor of the parent class (nn.Module) to set up necessary functionality.\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride = 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride = 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride = 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(32, 1, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # The forward method defines the computation that happens when the model is called with input x.\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loaders\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float()) # Each element in train_dataset will be a tuple (input, target). Both will have shape (400,400). There will be as many elements in the dataset as there are samples in X_train.\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float())\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset), len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) # model.parameters() passes the parameters of the model to the optimizer so that it can update them during training.\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    " \n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
    "       \n",
    "print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = model.to(device)\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data in train_loader:\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad() # In PyTorch, gradients are accumulated by default. This means that when you call loss.backward(), the gradients computed for each parameter are added to any existing gradients stored in those parameters. You want to reset the gradients at the start of each training step to avoid mixing gradient information from different batches.\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader) # Average loss over all batches\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, targets = data\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item()\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, _ = data\n",
    "        inputs= inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs)\n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "print(\"Predictions Shape:\", predictions.shape)\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_batch_size = 128   # Batch size for training\n",
    "value_epochs = 5\n",
    "value_learning_rate = 1e-3\n",
    "value_callbacks=[\n",
    "    tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True, mode='max'),\n",
    "    tfk.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.1, patience=15, min_lr=1e-5, mode='max')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x = X_train, \n",
    "    y = y_train,\n",
    "    batch_size = value_batch_size,\n",
    "    epochs = value_epochs,\n",
    "    validation_data = (X_val,y_val),\n",
    "    verbose=1,\n",
    "    callbacks = value_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Display the shape of the predictions\n",
    "print(\"Predictions Shape:\", predictions.shape)\n",
    "\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
